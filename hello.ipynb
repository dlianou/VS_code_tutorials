{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.6 64-bit ('.myenv')",
   "display_name": "Python 3.8.6 64-bit ('.myenv')",
   "metadata": {
    "interpreter": {
     "hash": "1b6cb9efb9ccc66904cb3903cd855c7664a9128800c8485238500de2fffeda3c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing datda\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data=data.astype({\"age\": np.float64, \"fare\": np.float64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visuzalizing variabe relationships\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(ncols=5, figsize=(30,5))\n",
    "sns.violinplot(x=\"survived\", y=\"age\", hue=\"sex\", data=data, ax=axs[0])\n",
    "sns.pointplot(x=\"sibsp\", y=\"survived\", hue=\"sex\", data=data, ax=axs[1])\n",
    "sns.pointplot(x=\"parch\", y=\"survived\", hue=\"sex\", data=data, ax=axs[2])\n",
    "sns.pointplot(x=\"pclass\", y=\"survived\", hue=\"sex\", data=data, ax=axs[3])\n",
    "sns.violinplot(x=\"survived\", y=\"fare\", hue=\"sex\", data=data, ax=axs[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for correlations in the data\n",
    "# all var's need to be converted to numeric\n",
    "data.replace({'male': 1, 'female':0}, inplace=True)\n",
    "# correlate the relationship between all variables and survival.\n",
    "data.corr().abs()[['survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesize that sibsp and parch are related and affect survivability, and group them into a new column called \"relatives\" to see whether the combination of them has a higher correlation to survivability. \n",
    "data['relatives'] = data.apply(lambda row: int((row['sibsp'] + row['parch']) >0), axis=1)\n",
    "data.corr().abs()['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given that a person with relatives have a higher correlation of surviving then those w/o, we can drop the low corr calues\n",
    "# of sibsp and parch. \n",
    "# We will also be dropping all NaN values\n",
    "data = data[['sex','pclass','age','relatives','fare','survived']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the model\n",
    "\n",
    "#Split the data into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data[['sex','pclass','age','relatives','fare']], data.survived,\n",
    "test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to normalize the features so they are all treated equally. For example, within the dataset the values for age range from ~0-100, while gender is only a 1 or 0.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(x_train)\n",
    "X_test = sc.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model selection\n",
    "# We wil use Naives Bayes, a common algo for classification problems\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.fit(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test the model against the test set\n",
    "from sklearn import metrics\n",
    "predict_test = model.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the same procedure using Neural Networks instead. Unlike the machine learning algorithm you looked at previously, neural networks are a form of deep learning wherein you don't need to know an ideal algorithm for your problem set ahead of time. \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: &quot;sequential_1&quot;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_15 (Dense)             (None, 5)                 30        \n_________________________________________________________________\ndense_16 (Dense)             (None, 5)                 30        \n_________________________________________________________________\ndense_17 (Dense)             (None, 1)                 6         \n=================================================================\nTotal params: 66\nTrainable params: 66\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# add the layers of the neural network. For now, let's keep things simple and just use three layers. Add the following code to create the layers of the neural network.\n",
    "model.add(Dense(5, kernel_initializer= 'uniform', activation='relu', input_dim = 5)) # Layer 1\n",
    "model.add(Dense(5, kernel_initializer= 'uniform', activation='relu')) # Layer 2\n",
    "model.add(Dense(1, kernel_initializer= 'uniform', activation='sigmoid')) # Layer 3\n",
    "\n",
    "# the sigmoid activation function is required for the final layer as the output you want (of whether a passenger survives or not) needs to be scaled in the range of 0-1 (the probability of a passenger surviving).\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/50\n27/27 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.5885\nEpoch 2/50\n27/27 [==============================] - 0s 2ms/step - loss: 0.6898 - accuracy: 0.5873\nEpoch 3/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.6847 - accuracy: 0.6914\nEpoch 4/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.6736 - accuracy: 0.7715\nEpoch 5/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.6553 - accuracy: 0.7727\nEpoch 6/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.6351 - accuracy: 0.7620\nEpoch 7/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.6157 - accuracy: 0.7644\nEpoch 8/50\n27/27 [==============================] - 0s 2ms/step - loss: 0.6016 - accuracy: 0.7667\nEpoch 9/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.5911 - accuracy: 0.7691\nEpoch 10/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.5820 - accuracy: 0.7727\nEpoch 11/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.5750 - accuracy: 0.7715\nEpoch 12/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.5680 - accuracy: 0.7656\nEpoch 13/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.5621 - accuracy: 0.7656\nEpoch 14/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.5564 - accuracy: 0.7644\nEpoch 15/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.5515 - accuracy: 0.7679\nEpoch 16/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.5461 - accuracy: 0.7715\nEpoch 17/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.5419 - accuracy: 0.7667\nEpoch 18/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.5379 - accuracy: 0.7691\nEpoch 19/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.5332 - accuracy: 0.7787\nEpoch 20/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.5297 - accuracy: 0.7775\nEpoch 21/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.5254 - accuracy: 0.7823\nEpoch 22/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.5219 - accuracy: 0.7859\nEpoch 23/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.5187 - accuracy: 0.7859\nEpoch 24/50\n27/27 [==============================] - 0s 2ms/step - loss: 0.5156 - accuracy: 0.7835\nEpoch 25/50\n27/27 [==============================] - 0s 2ms/step - loss: 0.5128 - accuracy: 0.7871\nEpoch 26/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.5103 - accuracy: 0.7847\nEpoch 27/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.5081 - accuracy: 0.7835\nEpoch 28/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.5059 - accuracy: 0.7871\nEpoch 29/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.5031 - accuracy: 0.7859\nEpoch 30/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.5009 - accuracy: 0.7871\nEpoch 31/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.4997 - accuracy: 0.7847\nEpoch 32/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.4972 - accuracy: 0.7835\nEpoch 33/50\n27/27 [==============================] - 0s 2ms/step - loss: 0.4953 - accuracy: 0.7871\nEpoch 34/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.4935 - accuracy: 0.7883\nEpoch 35/50\n27/27 [==============================] - 0s 2ms/step - loss: 0.4921 - accuracy: 0.7907\nEpoch 36/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.4902 - accuracy: 0.7895\nEpoch 37/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.4891 - accuracy: 0.7883\nEpoch 38/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.4875 - accuracy: 0.7883\nEpoch 39/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.4864 - accuracy: 0.7847\nEpoch 40/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.4856 - accuracy: 0.7847\nEpoch 41/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.4839 - accuracy: 0.7883\nEpoch 42/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.4825 - accuracy: 0.7859\nEpoch 43/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.4814 - accuracy: 0.7859\nEpoch 44/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.4805 - accuracy: 0.7835\nEpoch 45/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.4795 - accuracy: 0.7835\nEpoch 46/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.4787 - accuracy: 0.7883\nEpoch 47/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.4778 - accuracy: 0.7871\nEpoch 48/50\n27/27 [==============================] - 0s 1ms/step - loss: 0.4772 - accuracy: 0.7859\nEpoch 49/50\n27/27 [==============================] - 0s 2ms/step - loss: 0.4763 - accuracy: 0.7847\nEpoch 50/50\n27/27 [==============================] - 0s 3ms/step - loss: 0.4748 - accuracy: 0.7871\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tensorflow.python.keras.callbacks.History at 0x1b8cd973550&gt;"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "# Next we need to define the type of optimizer to use, how to calculate loss and what metric to optimize\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss = \"binary_crossentropy\", metrics=['accuracy'])\n",
    "model.fit(X_train,y_train, batch_size=32, epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From &lt;ipython-input-42-cce9eabd010a&gt;:2: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\nInstructions for updating:\nPlease use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) &gt; 0.5).astype(&quot;int32&quot;)`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n0.7751196172248804\n"
    }
   ],
   "source": [
    "# Test the model against the test set\n",
    "y_pred = model.predict_classes(X_test)\n",
    "print(metrics.accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}